{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jonathan Reus, 2018\n",
    "Modified from Sophie Li, 2016\n",
    "[http://blog.justsophie.com/python-speech-to-text-with-pocketsphinx/](http://blog.justsophie.com/python-speech-to-text-with-pocketsphinx/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pocketsphinx.pocketsphinx import Decoder\n",
    "#from sphinxbase.sphinxbase import *\n",
    "\n",
    "import os\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wave\n",
    "import audioop\n",
    "from collections import deque\n",
    "import time\n",
    "import math\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Store/Drive/DEV/almat/Pocketsphinx/models /Volumes/Store/Drive/DEV/almat/corpus\n"
     ]
    }
   ],
   "source": [
    "# Create a pocketsphinx decoder\n",
    "\n",
    "GO = False\n",
    "os.getcwd()\n",
    "# Set up pocketsphinx decoder\n",
    "MODELDIR = os.path.normpath(os.getcwd() + \"/models/\")\n",
    "DATADIR = os.path.normpath(os.getcwd() + \"/../corpus/\")\n",
    "print(MODELDIR, DATADIR)\n",
    "\n",
    "# Create a decoder with certain model\n",
    "config = Decoder.default_config()\n",
    "config.set_string('-hmm', os.path.join(MODELDIR, 'en-us/en-us'))\n",
    "config.set_string('-lm', os.path.join(MODELDIR, 'en-us/en-us.lm.bin'))\n",
    "config.set_string('-dict', os.path.join(MODELDIR, 'en-us/cmudict-en-us.dict'))\n",
    "\n",
    "# Creates the decoder object\n",
    "decoder = Decoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 Built-in Microphone, Core Audio (2 in, 0 out)\n",
      "   1 Built-in Output, Core Audio (0 in, 2 out)\n",
      "   2 Soundflower (2ch), Core Audio (2 in, 2 out)\n",
      "   3 Soundflower (64ch), Core Audio (64 in, 64 out)\n",
      "   4 Premiere Pro 5.0, Core Audio (0 in, 0 out)\n",
      "*  5 Fireface UCX (23590637), Core Audio (18 in, 18 out)\n",
      "   6 H2Core, Core Audio (0 in, 2 out)\n",
      "   7 USBMixer, Core Audio (0 in, 0 out)\n",
      "   8 Soundblaster PLAY!, Core Audio (0 in, 0 out)\n",
      "   9 Builtin+SF, Core Audio (64 in, 66 out)\n",
      "  10 Saffire+SF, Core Audio (64 in, 64 out)\n",
      "  11 FA101+SF, Core Audio (64 in, 64 out)\n",
      "['float32', 'float32']\n",
      "Default:  [5, 5]\n"
     ]
    }
   ],
   "source": [
    "#DEV = sd.default.device\n",
    "INDEV = 5   # set to input device\n",
    "OUTDEV = 5  # set to output device\n",
    "sd.default.device = [INDEV,OUTDEV];\n",
    "print(sd.query_devices())\n",
    "print(sd.default.dtype)\n",
    "print(\"Default: \",sd.default.device) # sd.default.device is a property that can be set\n",
    "RATE = 16000.0\n",
    "BLOCK = 512\n",
    "DTYPE = 'int16'\n",
    "NUMCHANS = 1\n",
    "LATENCY = ('low','high') # see https://python-sounddevice.readthedocs.io/en/0.3.12/api.html#sounddevice.default.dtype\n",
    "LATENCY = 0.1\n",
    "NUM_PHRASES = -1 # ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a blocking audiostream\n",
    "# See: https://python-sounddevice.readthedocs.io/en/0.3.12/api.html#sounddevice.Stream.read\n",
    "stream = sd.InputStream(device=INDEV, samplerate=RATE, latency=LATENCY, blocksize=BLOCK, dtype=DTYPE, channels=NUMCHANS)\n",
    "stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the audio stream\n",
    "stream.stop()\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run audio analysis. Use a blocking stream!\n",
    "\n",
    "SILENCE_LIMIT = 1 # Silence limit in seconds. The max ammount of seconds where\n",
    "                           # only silence is recorded. When this time passes the\n",
    "                           # recording finishes and the audio buffer is decoded\n",
    "PREV_AUDIO = 0.5  # Previous audio (in seconds) to prepend. When noise\n",
    "                  # is detected, how much of previously recorded audio is\n",
    "                  # prepended. This helps to prevent chopping the beginning\n",
    "                  # of the phrase.\n",
    "GO = 0\n",
    "STARTED = False\n",
    "THRESHOLD = 300 # RMS value\n",
    "audio2send = [] # expanding list of numpy arrays, each a block of audio to be processed\n",
    "cur_data = ''  # current chunk of audio data\n",
    "ratio = RATE / BLOCK\n",
    "# sliding window stores 1s worth of blocks' RMS values, used as a moving RMS window to detect silence / end of utterance\n",
    "slid_win = deque(maxlen=ceil(SILENCE_LIMIT * ratio))\n",
    "# a deque of blocks, stores 0.5 seconds of audio before the threshhold is triggered for. Used to prevent chopping at the beginning of an utterance.\n",
    "prev_audio = deque(maxlen=ceil(PREV_AUDIO * ratio)) \n",
    "started = False\n",
    "lost_data = False\n",
    "\n",
    "# listen to 3 seconds of audio\n",
    "while GO < (3 * ratio):\n",
    "    # get some data as a bytes-like object from the mic    \n",
    "    # sd.read returns a numpy.ndarray with one column per channel (frames, channels) \n",
    "    cur_data,overflow = stream.read(BLOCK)\n",
    "    \n",
    "    # get rms over all samples in the fragment, add RMS value to sliding window (1s worth of blocks)\n",
    "    # audioop provides simple operations on sound fragments stored as python strings\n",
    "    # see: https://docs.python.org/2/library/audioop.html\n",
    "    slid_win.append(audioop.rms(cur_data[:,0], 2))\n",
    "    thesum = sum([x > THRESHOLD for x in slid_win]) # number of blocks whose RMS is above a given threshhold\n",
    "    print(\"slid_win is\", slid_win, \"...datalost?\", overflow)\n",
    "    if thesum > 0: # more than one block has sqrt(avg) over threshhold, so we haven't hit silence yet\n",
    "        if STARTED == False:\n",
    "            print(\"Starting recording of utterance\")\n",
    "            STARTED = True\n",
    "        audio2send.append(cur_data[:,0]) # append current data block to what will be sent for analysis\n",
    "    elif STARTED:\n",
    "        # We were recording, but there has been too much silence.\n",
    "        print(\"Finished recording, decoding phrase. Started is: \", STARTED) # enough silence has passed...\n",
    "\n",
    "        # concat previous 0.5s + recorded blocks into a single buffer\n",
    "        buffer = np.concatenate(list(prev_audio) + audio2send)  \n",
    "        # Play phrase out the speaker\n",
    "        sd.play(buffer, samplerate=RATE, blocking=False, device=OUTDEV)\n",
    "        \n",
    "        # Decode using pocketsphinx\n",
    "        decoder.start_utt() # begin processing utterance\n",
    "        decoder.process_raw(buffer, False, False)\n",
    "        #decoder.process_cep(buffer, False, False) # process cepstrum data\n",
    "        decoder.end_utt()\n",
    "        words = []\n",
    "        [words.append(seg.word) for seg in decoder.seg()]\n",
    "        print(words)\n",
    "        \n",
    "        # Save audio utterance as file and send to sphinx.\n",
    "        #filename = save_speech(list(prev_audio) + audio2send, p)\n",
    "        #r = decode_phrase(filename)\n",
    "        #print(\"DETECTED: \", r)\n",
    "\n",
    "        # Get ready for the next audio block.\n",
    "        STARTED = False\n",
    "        slid_win = deque(maxlen=ceil(SILENCE_LIMIT * ratio))\n",
    "        prev_audio = deque(maxlen=ceil(0.5 * ratio))\n",
    "        audio2send = []\n",
    "        print(\"Listening ...\")\n",
    "    else:\n",
    "        # There is silence and we are not yet in the middle of recording an utterance..\n",
    "        prev_audio.append(cur_data[:,0]) # why...?\n",
    "        print(\"Silence ...\")\n",
    "    GO += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print \"* Done listening\"\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "* [CmuSphinx Docs for ps_seg](https://cmusphinx.github.io/doc/pocketsphinx/structps__seg__s.html)\n",
    "* [PocketSphinx Decoder Docs](https://cmusphinx.github.io/doc/python/pocketsphinx.pocketsphinx.Decoder-class.html)\n",
    "* [Decoder Test Using Python](https://github.com/cmusphinx/pocketsphinx/blob/master/swig/python/test/decoder_test.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the Data After Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg in decoder.seg():\n",
    "    print(\"WORD:\",seg.word, \"Acoustic-score:\", seg.ascore, \" Language-score:\", seg.lscore, \"Log Posterior Probability:\", seg.prob)\n",
    "    print(seg.start_frame, seg.end_frame, seg.lback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HH AH L OW\n",
      "HH EH L OW\n",
      "L AH V\n",
      "Y UW\n"
     ]
    }
   ],
   "source": [
    "# Find pronounciations in phoneme->word dictionary\n",
    "print(decoder.lookup_word(\"hello\"))\n",
    "print(decoder.lookup_word(\"hello(2)\"))\n",
    "print(decoder.lookup_word(\"love\"))\n",
    "print(decoder.lookup_word(\"you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'hypstr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-60109f93d90c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogmath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logmath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Best hypothesis: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" model score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" confidence: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Best hypothesis segments: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'hypstr'"
     ]
    }
   ],
   "source": [
    "# Decoder hypothesis...\n",
    "hypothesis = decoder.hyp()\n",
    "logmath = decoder.get_logmath()\n",
    "print ('Best hypothesis: ', hypothesis.hypstr, \" model score: \", hypothesis.best_score, \" confidence: \", logmath.exp(hypothesis.prob))\n",
    "print ('Best hypothesis segments: ', [seg.word for seg in decoder.seg()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 hypothesis: \n",
      "in -21659\n"
     ]
    }
   ],
   "source": [
    "# Access N best decodings.\n",
    "print ('Best 10 hypothesis: ')\n",
    "for best, i in zip(decoder.nbest(), range(10)):\n",
    "    print (best.hypstr, best.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
